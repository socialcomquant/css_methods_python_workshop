{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python\n",
    "\n",
    "### Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this Python notebook</b>, \n",
    "\n",
    "we will explore how to perform text preprocessing on a corpus of tweets. Text preprocessing is a critical task in natural language processing (NLP), which involves cleaning and transforming raw text data into a format that can be analyzed by NLP algorithms. In this notebook, we will focus on several specific techniques for text preprocessing, including tokenization, lemmatization, stemming, part of speech tagging, stop word removal, and n-grams extraction.\n",
    "\n",
    "By the end of this notebook, you will have a basic understanding of how to perform text preprocessing on text data, Let's get started!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emoji</th>\n",
       "      <th>emoji_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1265465820995411973</td>\n",
       "      <td>This was me, and I want to make one thing clea...</td>\n",
       "      <td>257467</td>\n",
       "      <td>['https://t.co/349TZijtD8']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>This was me, and I want to make one thing clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1266553959973445639</td>\n",
       "      <td>Mike Pence caught on hot mic delivering empty ...</td>\n",
       "      <td>135818</td>\n",
       "      <td>['https://t.co/IduvGhiPwj']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mike Pence caught on hot mic delivering empty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1258750892448387074</td>\n",
       "      <td>THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...</td>\n",
       "      <td>88667</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1263579286201446400</td>\n",
       "      <td>This just happened on live tv. Wow, what a dou...</td>\n",
       "      <td>82495</td>\n",
       "      <td>['https://t.co/dQKheEcCvb']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>This just happened on live tv. Wow, what a dou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1266546753182056453</td>\n",
       "      <td>Mask on</td>\n",
       "      <td>66604</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mask on</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1265465820995411973  This was me, and I want to make one thing clea...   \n",
       "1  1266553959973445639  Mike Pence caught on hot mic delivering empty ...   \n",
       "2  1258750892448387074  THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...   \n",
       "3  1263579286201446400  This just happened on live tv. Wow, what a dou...   \n",
       "4  1266546753182056453                                            Mask on   \n",
       "\n",
       "   retweets                         urls mentions hashtags emoji emoji_text  \\\n",
       "0    257467  ['https://t.co/349TZijtD8']       []       []    []         []   \n",
       "1    135818  ['https://t.co/IduvGhiPwj']       []       []    []         []   \n",
       "2     88667                           []       []       []    []         []   \n",
       "3     82495  ['https://t.co/dQKheEcCvb']       []       []    []         []   \n",
       "4     66604                           []       []       []    []         []   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  This was me, and I want to make one thing clea...  \n",
       "1  Mike Pence caught on hot mic delivering empty ...  \n",
       "2  THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...  \n",
       "3  This just happened on live tv. Wow, what a dou...  \n",
       "4                                            Mask on  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import the data \n",
    "tweets_df = pd.read_csv('../data/top_500_retweeted_tweets_clean.csv', encoding = \"utf-8\")\n",
    "tweets_df.head() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Tokenization\n",
    "\n",
    "The set of all the unique terms in our data is called the vocabulary. Each element in this set is called a type. Each occurrence of a type in the data is called a token. \n",
    "\n",
    "Let's practice! Our sentence is\n",
    "\n",
    ">“Today is a great day to learn NLP, such a powerful tool!”\n",
    "\n",
    "Thi sentece has 14 tokens but only 13 types (namely, 'Today', 'is', 'a', 'great', 'day', 'to', 'learn', 'NLP', ',', 'such', 'a', 'powerful', 'tool', '!'). Note that types can also include punctuation marks and multiword expressions.\n",
    "\n",
    "In other words, the words of a text document/file separated by spaces and punctuation are called as tokens.\n",
    "\n",
    "#### What is a Tokenization?\n",
    "The process of extracting tokens from a text file/document is referred as tokenization. Let's see an example below of a tokenization process using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt. \n",
      "\n",
      "Tokens in the text:\n",
      "\t Mike\n",
      "\t Pence\n",
      "\t caught\n",
      "\t on\n",
      "\t hot\n",
      "\t mic\n",
      "\t delivering\n",
      "\t empty\n",
      "\t boxes\n",
      "\t of\n",
      "\t PPE\n",
      "\t for\n",
      "\t a\n",
      "\t PR\n",
      "\t stunt\n",
      "\t .\n",
      "\n",
      "Total tokens: 16\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the original and tokenized text\n",
    "print('Original text:', text)\n",
    "print('\\nTokens in the text:',)\n",
    "\n",
    "for token in doc:\n",
    "    print('\\t', token.text)\n",
    "\n",
    "print('\\nTotal tokens:', len(doc))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also push further our analysis and extract the vocabulary from the corpus of tweets from the previous dataset. Since the vocabulary of a text corpus is the collection of unique tokens present in that corpus, we will just need to tokenize each single tweet and keep unique occurence of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of extracted vocabulary: 0\n",
      "Total number of tokens: 0\n",
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Process all tweets with spaCy and extract all tokens\n",
    "tokens = []\n",
    "\n",
    "# iterate through all tweets and extract all tokens\n",
    "###\n",
    "\n",
    "# Count the occurrences of each token and create a vocabulary of unique tokens\n",
    "vocabulary = Counter(tokens)\n",
    "\n",
    "# Print the extracted vocabulary\n",
    "print(\"Size of extracted vocabulary: {0}\".format(len(vocabulary)))\n",
    "print(\"Total number of tokens: {0}\".format(len(tokens)))\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "When we look up a word in a dictionary, we usually just look for the base form. This dictionary base form is called the **lemma**.\n",
    "For instance, we might see forms like “go”, “goes”, “went”, “gone”, or “going” and we look up dictionary in a lemmatized form, such as \"go\" (Hovy, 2020). These words have clearly different meaning, in some contexts it is not fundamental to distinguish them. On the contrary, it is much more convenient to trace them back to their lemma. Indeed, this may simplify some analysis and allow easier extraction of relevant information from the text. Let's see an example of lemmatization applied to the corpus of tweets using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike -> Mike\n",
      "Pence -> Pence\n",
      "caught -> catch\n",
      "on -> on\n",
      "hot -> hot\n",
      "mic -> mic\n",
      "delivering -> deliver\n",
      "empty -> empty\n",
      "boxes -> box\n",
      "of -> of\n",
      "PPE -> PPE\n",
      "for -> for\n",
      "a -> a\n",
      "PR -> pr\n",
      "stunt -> stunt\n",
      ". -> .\n",
      "\n",
      "\n",
      "Original text: Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt. \n",
      "Lemmatized text: Mike Pence catch on hot mic deliver empty box of PPE for a pr stunt .\n"
     ]
    }
   ],
   "source": [
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy and perform lemmatization\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print words and extractes lemmas\n",
    "for token in doc:\n",
    "    print(\"{0} -> {1}\".format(token.text, token.lemma_))\n",
    "\n",
    "# Finally we can recover the text of the tweet after lemmatization\n",
    "print('\\n\\nOriginal text:', text)\n",
    "lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "print('Lemmatized text:', lemmatized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "\n",
    "Another strategy to reduce different forms of a word to a common base or root form is stemming. Stemming involves removing the suffixes of words to create a simplified form of the word. For example, the stem of the words \"running,\" \"runner,\" and \"run\" is \"run.\" This can be achieved using several algorithms like the one developed by Porter (1980). This algorithm defines a number of suffixes and the order in which they should be removed or replaced. These actions are then applied iteratively untill a word is reduced to its stem.\n",
    "\n",
    "Note how, although similar, stemming and lemmatization are different and give different results. Generally speaking, lemmatization tends to produce more accurate and meaningful results with respect to stemming. Nonethelss, stemming is often faster and simpler to implement, which makes it useful for tasks that require real-time processing or have limited computational resources.\n",
    "\n",
    "An implementation of the Porter stemmer is available in the Python library NLTK. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to install NLTK\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/nicolo/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "# download popular NLTK data\n",
    "!python -m nltk.downloader popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike -> mike\n",
      "Pence -> penc\n",
      "caught -> caught\n",
      "on -> on\n",
      "hot -> hot\n",
      "mic -> mic\n",
      "delivering -> deliv\n",
      "empty -> empti\n",
      "boxes -> box\n",
      "of -> of\n",
      "PPE -> ppe\n",
      "for -> for\n",
      "a -> a\n",
      "PR -> pr\n",
      "stunt -> stunt\n",
      ". -> .\n",
      "\n",
      "\n",
      "Original text: Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt. \n",
      "Stemmed text: mike penc caught on hot mic deliv empti box of ppe for a pr stunt .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# This performs tokenization on the text (NLTK equivalalent of what we did with spaCy)\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Create a PorterStemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word in the text\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Let's see results \n",
    "for token, stem in zip(tokens, stemmed_tokens):\n",
    "    print(\"{0} -> {1}\".format(token, stem))\n",
    "\n",
    "# Finally we can recover the text of the tweet after lemmatization\n",
    "print('\\n\\nOriginal text:', text)\n",
    "stemmed_text = \" \".join(stemmed_tokens)\n",
    "print('Stemmed text:', stemmed_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "In natural language processing (NLP), **N-grams** are contiguous sequences of n elements from a given text sample, where an element can be a word, a character, or part of speech. In most cases, n-grams are created from a text by dragging a window of size n over the text and extracting the sequences of n elements that fall within that window.\n",
    "\n",
    "N-grams are used in a variety of NLP tasks such as language modeling, machine translation, and text classification. By extracting n-grams from a text, it is possible to capture the local context of a word or word sequence, which can help improve the accuracy of many NLP tasks.\n",
    "\n",
    "For example, a bigram (n=2) is \"natural language\", a trigram (n=3) is \"natural language processing\", and a 4-gram (n=4) is \"natural language processing task\". By examining the frequency of different n-grams in a text or corpus, it is possible to gain insight into the distribution of words and their relationships.\n",
    "\n",
    "N-grams can also be used to generate new texts through techniques such as n-gram language modeling. In this approach, the probabilities of different N-grams in a text are used to generate a new text that is similar in style and content to the original text.\n",
    "\n",
    "However, it should be noted that n-grams can be constrained by the sparsity problem, especially for larger values of n. That is, as the value of n increases, the number of unique n-grams in a text can increase rapidly, making it difficult to capture meaningful patterns or relationships. Therefore, choosing an appropriate value of n is an important consideration in many NLP tasks.\n",
    "\n",
    "Let's see an example of  N-grams extraction applied to the corpus of tweets using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: ['Mike', 'Pence', 'caught', 'on', 'hot', 'mic', 'delivering', 'empty', 'boxes', 'of', 'PPE', 'for', 'a', 'PR', 'stunt', '.']\n",
      "Bigrams: ['Mike Pence', 'Pence caught', 'caught on', 'on hot', 'hot mic', 'mic delivering', 'delivering empty', 'empty boxes', 'boxes of', 'of PPE', 'PPE for', 'for a', 'a PR', 'PR stunt', 'stunt .']\n",
      "Trigrams: ['Mike Pence caught', 'Pence caught on', 'caught on hot', 'on hot mic', 'hot mic delivering', 'mic delivering empty', 'delivering empty boxes', 'empty boxes of', 'boxes of PPE', 'of PPE for', 'PPE for a', 'for a PR', 'a PR stunt', 'PR stunt .']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define the function to extract n-grams\n",
    "def extract_ngrams(doc, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(doc) - n + 1):\n",
    "        ngram = \" \".join([doc[j].text for j in range(i, i + n)])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Extract unigrams, bigrams, and trigrams from the text\n",
    "unigrams = extract_ngrams(doc, 1)\n",
    "bigrams = extract_ngrams(doc, 2)\n",
    "trigrams = extract_ngrams(doc, 3)\n",
    "\n",
    "# Print the extracted n-grams\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use Gensim, another popular library for NLP, to automatically extract the most common n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tear_gas', 'United_States', 'BREAKING_:', 'White_House', '_Candy', 'George_Floyd', 'IS_STILL', '_Truth_Or_Dare', 'Dr._Fauci', '큥이_에리_기가막힌_케미스트리', 'social_distancing', '&_amp', 'THE_PANDEMIC'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# gensim expect as input tokenized texts\n",
    "texts = []\n",
    "for text in tweets_df.cleaned_text.values:\n",
    "    texts.append(word_tokenize(text))\n",
    "\n",
    "# extract bigrams\n",
    "bigrams = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "texts_bigrams = [bigrams[text] for text in texts]\n",
    "\n",
    "# visualize the extracted bigrams\n",
    "extracted_bigrams = []\n",
    "for text in texts_bigrams:\n",
    "    for el in text:\n",
    "        if \"_\" in el:\n",
    "            extracted_bigrams.append(el)\n",
    "\n",
    "extracted_bigrams = set(extracted_bigrams)\n",
    "print(extracted_bigrams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2. Stopwords\n",
    "\n",
    "In natural language processing (NLP), stop words refer to words that are frequently used in a language but usually do not have much meaning or semantic value when used in context. Examples of stop words in English are \"the\", \"a\", \"an\", \"and\", \"in\", \"on\", \"is\", \"are\", \"for\", \"with\", and so on.\n",
    "\n",
    "Stop words are usually removed from text during preprocessing in NLP tasks such as text classification, sentiment analysis, and information retrieval. The reason is that they do not contribute much to the overall meaning or topic of a text and can potentially degrade algorithm performance by adding noise to the data. Removing stop words can also help reduce the size of vocabulary and improve the efficiency of text processing algorithms.\n",
    "\n",
    "However, there are certain cases where the inclusion of stop words in the analysis may be useful or even necessary. For example, stopwords can be useful in tasks such as authorship attribution, to identify common themes, or writing styles. In such cases, it is important to carefully consider the use of stop words and their potential impact on the analysis\n",
    "\n",
    "We will now see a simple example on how to remove Stop Words from a text using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens:  ['Mike', 'Pence', 'caught', 'on', 'hot', 'mic', 'delivering', 'empty', 'boxes', 'of', 'PPE', 'for', 'a', 'PR', 'stunt', '.']\n",
      "Filtered tokens: ['Mike', 'Pence', 'caught', 'hot', 'mic', 'delivering', 'boxes', 'PPE', 'PR', 'stunt', '.']\n",
      "\n",
      "Stop words removed:  ['on', 'empty', 'of', 'for', 'a']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the small English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define the list of stop words\n",
    "stop_words = list(STOP_WORDS)\n",
    "\n",
    "# Remove stop words from the text\n",
    "filtered_text = [token.text for token in doc if token.text not in stop_words]\n",
    "stop_words_removed = [token.text for token in doc if token.text in stop_words]\n",
    "\n",
    "# Print the original and filtered text, and the stop words removed\n",
    "print(\"Original tokens: \", [token.text for token in doc])\n",
    "print(\"Filtered tokens:\", filtered_text)\n",
    "print(\"\\nStop words removed: \", stop_words_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common stop word is: the\n"
     ]
    }
   ],
   "source": [
    "# exercise: find the most common stop word\n",
    "\n",
    "# Define a dictionary to store the count of each stop word\n",
    "stop_words_count = {}\n",
    "\n",
    "# iterate over tweets\n",
    "\n",
    "    # find stop words in this text\n",
    "    \n",
    "    # iterate over stop words found and update counts\n",
    "    \n",
    "\n",
    "# Find the stop word with the highest count    \n",
    "most_frequent_stop_word = max(stop_words_count, key=stop_words_count.get)\n",
    "print(\"The most common stop word is:\", most_frequent_stop_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the task, one can also add custom stop words. This can be easily done by appending additional words to the stop words list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words))\n",
    "stop_words.append(\"place\")\n",
    "print(len(stop_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3. Parts of Speech\n",
    "\n",
    "**Part of speech tagging** (POS) is the process of assigning a part of speech to each word in a sentence, such as noun, verb, adjective, or adverb. POS Tagging is an important step in many NLP applications, such as named entity recognition, sentiment analysis, and machine translation.\n",
    "\n",
    "The goal of POS tagging is to identify the grammatical structure of a sentence by labelling each word with its corresponding part of speech. This information can then be used to extract meaning and context from the text. For example, knowing whether a word is a noun or a verb can help determine the subject and predicate of a sentence.\n",
    "\n",
    "POS tagging is typically performed using machine learning algorithms, such as hidden Markov models, conditional random fields, or neural networks. These algorithms are trained on annotated text corpora in which each word is labelled with the corresponding word type. After training, the algorithm can then predict the word type for a new unseen text.\n",
    "\n",
    "POS tagging is not always an easy task, as some words may have multiple possible word types depending on the context. For example, \"run\" can be a verb (\"I run every morning\") or a noun (\"I went for a run\"). In these cases, the algorithm must use contextual clues to determine the most likely part of speech for the word.\n",
    "\n",
    "Overall, POS tagging is an important technique in NLP that helps extract meaning and context from texts by identifying the grammatical structure of sentences.\n",
    "\n",
    "English has 9 main categories:\n",
    "\n",
    "- verb — Expresses an action or a state of being. E.g. jump, is, write, become\n",
    "- noun — identifies a person, a place or a thing or names of particular of one of these (pronoun). E.g. man, house, happiness\n",
    "- pronoun — can replace a noun or noun phrase. E.g. she, we, they, it\n",
    "- determiner — Is placed in front of a noun to express a quantity or clarify what the noun refers to — briefly a noun introducer. E.g. my, that, the, many\n",
    "- adjective — modifies a noun or a pronoun. E.g. pretty, old, blue, smart\n",
    "- adverb — modifies a verb, an adjective, or another adverb. E.g. gently, extremely, carefully, well\n",
    "- preposition — Connect a noun/pronoun to other parts of the sentence. E.g. by, with, about, until\n",
    "- conjunction — glue words, clauses, and sentences together. E.g. and, but, or, while, because\n",
    "- interjection — Expresses emotion in a sudden or exclamatory way. E.g. oh!, wow!, oops!\n",
    "\n",
    "\n",
    "<img src='../data/POS.png' style='height: 200px; float: center'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will now see a simple example on how to perform POS on a text using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike PROPN\n",
      "Pence PROPN\n",
      "caught VERB\n",
      "on ADP\n",
      "hot ADJ\n",
      "mic NOUN\n",
      "delivering VERB\n",
      "empty ADJ\n",
      "boxes NOUN\n",
      "of ADP\n",
      "PPE PROPN\n",
      "for ADP\n",
      "a DET\n",
      "PR NOUN\n",
      "stunt NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As a text example we will use a tweet from the previous dataset\n",
    "text = tweets_df.cleaned_text[1]\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# iterate over each token in the doc and print its text and POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the meaning of a POS tag is not clear to us, we ask spaCy to explain it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"PROPN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how spaCy POS tagging works on more tricky examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "run VERB\n",
      "every DET\n",
      "morning NOUN\n",
      "\n",
      "\n",
      "I PRON\n",
      "went VERB\n",
      "for ADP\n",
      "a DET\n",
      "run NOUN\n"
     ]
    }
   ],
   "source": [
    "# here the word run is used as verb\n",
    "text1 = \"I run every morning\"\n",
    "\n",
    "# here the word run is used as a noun\n",
    "text2 = \"I went for a run\"\n",
    "\n",
    "# POS tagging of sentence 1\n",
    "for token in nlp(text1):\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "print(\"\\n\")\n",
    "# POS tagging of sentence 2\n",
    "for token in nlp(text2):\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that spaCy correctly tag the word \"run\" differently in these two examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0-GgEFeNJbn"
   ],
   "name": "Day 1: Introduction to Jupyter Notebook and Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
