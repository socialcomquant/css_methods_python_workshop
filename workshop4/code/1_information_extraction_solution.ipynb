{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python\n",
    "\n",
    "### Natural Language Processing - Information Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this Python notebook</b>, \n",
    "\n",
    "we will explore how to use\n",
    "- regular expressions and, \n",
    "- named entity recognition (NER) \n",
    "\n",
    "to perform information extraction from a corpus of tweets. \n",
    "\n",
    "Information extraction is a critical task in natural language processing (NLP), which involves identifying and extracting relevant information from unstructured text data.\n",
    "\n",
    "Regular expressions are a powerful tool for pattern matching and text manipulation. They allow us to define complex patterns of characters and symbols that can match specific text patterns, such as email addresses, phone numbers, or URLs. Regular expressions are particularly useful for information extraction tasks that involve dates, times, or addresses.\n",
    "\n",
    "Named entity recognition (NER) is a technique for identifying and classifying named entities in text data, such as people, organizations, or locations. NER is a critical component of many NLP applications, such as information retrieval, question answering, and text summarization. \n",
    "\n",
    "Our corpus of tweets consist of a sample of $500$ tweets related to a specific topic or event.\n",
    "\n",
    "By the end of this notebook, you will have a basic understanding of how to use regular expressions and named entity recognition to perform information extraction from text data, and how to analyze and visualize the extracted information for insights and understanding. Let's get started!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1265465820995411973</td>\n",
       "      <td>This was me, and I want to make one thing clea...</td>\n",
       "      <td>257467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1266553959973445639</td>\n",
       "      <td>Mike Pence caught on hot mic delivering empty ...</td>\n",
       "      <td>135818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1258750892448387074</td>\n",
       "      <td>THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...</td>\n",
       "      <td>88667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1263579286201446400</td>\n",
       "      <td>This just happened on live tv. Wow, what a dou...</td>\n",
       "      <td>82495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1266546753182056453</td>\n",
       "      <td>Mask on</td>\n",
       "      <td>66604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1265465820995411973  This was me, and I want to make one thing clea...   \n",
       "1  1266553959973445639  Mike Pence caught on hot mic delivering empty ...   \n",
       "2  1258750892448387074  THE PANDEMIC IS STILL HAPPENING. THE PANDEMIC ...   \n",
       "3  1263579286201446400  This just happened on live tv. Wow, what a dou...   \n",
       "4  1266546753182056453                                            Mask on   \n",
       "\n",
       "   retweets  \n",
       "0    257467  \n",
       "1    135818  \n",
       "2     88667  \n",
       "3     82495  \n",
       "4     66604  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# import the data \n",
    "tweets_df = pd.read_csv('../data/top_500_retweeted_tweets.csv', encoding = \"utf-8\")\n",
    "tweets_df.head() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Extracting patterns using Regular Expressions\n",
    "\n",
    "<a href=\"https://docs.python.org/3/howto/regex.html\">Regular expressions</a> (also called regex, regexes, regex pattern, regexp, or REs) are a sequence of characters that define a search pattern. They are used in programming and text processing to match and manipulate strings of text based on a specific pattern.\n",
    "A regular expression is a pattern used to match one or more text strings. It is usually composed of a combination of characters, symbols, and metacharacters. Metacharacters are special characters that have a specific meaning in regular expressions, for example, the period (.) that matches any single character, or the asterisk (*) that matches zero or more occurrences of the preceding character.\n",
    "Regular expressions can be used to perform a variety of operations on text data, such as searching for specific patterns, replacing text with other text, or extracting specific information from a text string. \n",
    "Some common examples of regular expressions are matching an email addresses, phone numbers, dates, and URLs.\n",
    "\n",
    "Regular expressions can be complex and difficult to read, but they are a powerful tool for manipulating and processing text data. Luckily, there are many resources that can help us write the correct regular expression for our task. Also, Python has built-in mobule (`re`) to use regular expressions.\n",
    "\n",
    "<img src='../data/Regular_Expressions_Cheat_Sheet.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will extract all URLs from the text of the tweet. A possible regular expression to match an URL is:\n",
    "\n",
    "`http[s]*\\S+`\n",
    "\n",
    "This regular expression will match all strings that starts with `http`, or eventually with `https`, followed by non-empty spaces. \n",
    "\n",
    "We will use the `findall` function from the Python module `re` to match all URLs in text of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/349TZijtD8']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we create a new column where we store all the URLs mentioned in the tweet extracted using regex\n",
    "tweets_df['urls'] = tweets_df['text'].apply(lambda x: re.findall(\"http[s]*\\S+\", x))\n",
    "tweets_df['urls'].values[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract **mentions and hashtags** applying opportune regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@realDonaldTrump']\n"
     ]
    }
   ],
   "source": [
    "# find all mentions\n",
    "tweets_df['mentions'] = tweets_df['text'].apply(lambda x: re.findall(\"@[a-zA-Z0-9_]{1,50}\", x))\n",
    "print(tweets_df['mentions'].values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#COVID']\n"
     ]
    }
   ],
   "source": [
    "# excercise: find all hashtags\n",
    "# hashtags example: #soccomquant\n",
    "tweets_df['hashtags'] = tweets_df['text'].apply(lambda x: re.findall(\"#[a-zA-Z0-9_]{1,50}\", x))\n",
    "print(tweets_df['hashtags'].values[-5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **emoji** extraction, in addition to regex, we will use the library called emoji (if not installed before, please install it before running the following cell). This library helps us transform emojis into the related codes (i.e., texts). Once the emojis are converted to text, we apply the same logic applied so far with regex to find them. \n",
    "\n",
    "The full list of emojis and related codes is available here: https://unicode.org/emoji/charts/full-emoji-list.html\n",
    "\n",
    "Let's look at and example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':face_with_tears_of_joy:'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(\"üòÇ\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this approach to the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emoji</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1264986843948277760</td>\n",
       "      <td>People who say ‚Äòwell, he‚Äôs doing the best he c...</td>\n",
       "      <td>9033</td>\n",
       "      <td>[https://t.co/5POEhfB6vi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#COVID]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1260425005483073538</td>\n",
       "      <td>This young woman was killed in her home for no...</td>\n",
       "      <td>9021</td>\n",
       "      <td>[https://t.co/JzPgOzm4Rm]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#BreonnaTaylor]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1259587972728533000</td>\n",
       "      <td>I be like ‚Äúoh shit my mask‚Äù like I‚Äôm Batman or...</td>\n",
       "      <td>8994</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üòÇüòÇ]</td>\n",
       "      <td>[:face_with_tears_of_joy::face_with_tears_of_j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1266251584461090816</td>\n",
       "      <td>Really disappointed by @SAfridiOfficial‚Äòs comm...</td>\n",
       "      <td>8984</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@SAfridiOfficial, @narendramodi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üáÆüá≥]</td>\n",
       "      <td>[:India:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1266728243236950018</td>\n",
       "      <td>Let's be clear about what's happening:\\n\\n‚Üí Am...</td>\n",
       "      <td>8974</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@realDonaldTrump]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "495  1264986843948277760  People who say ‚Äòwell, he‚Äôs doing the best he c...   \n",
       "496  1260425005483073538  This young woman was killed in her home for no...   \n",
       "497  1259587972728533000  I be like ‚Äúoh shit my mask‚Äù like I‚Äôm Batman or...   \n",
       "498  1266251584461090816  Really disappointed by @SAfridiOfficial‚Äòs comm...   \n",
       "499  1266728243236950018  Let's be clear about what's happening:\\n\\n‚Üí Am...   \n",
       "\n",
       "     retweets                       urls                           mentions  \\\n",
       "495      9033  [https://t.co/5POEhfB6vi]                                 []   \n",
       "496      9021  [https://t.co/JzPgOzm4Rm]                                 []   \n",
       "497      8994                         []                                 []   \n",
       "498      8984                         []  [@SAfridiOfficial, @narendramodi]   \n",
       "499      8974                         []                 [@realDonaldTrump]   \n",
       "\n",
       "             hashtags emoji                                         emoji_text  \n",
       "495          [#COVID]    []                                                 []  \n",
       "496  [#BreonnaTaylor]    []                                                 []  \n",
       "497                []  [üòÇüòÇ]  [:face_with_tears_of_joy::face_with_tears_of_j...  \n",
       "498                []  [üáÆüá≥]                                          [:India:]  \n",
       "499                []    []                                                 []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_emojis(text, return_codes=False):\n",
    "    # first turn emojis into related text code\n",
    "    text_de = emoji.demojize(text)\n",
    "    # second find all emojis text code\n",
    "    emojis_list_de = re.findall(r'(:[\\a-z]+:)', text_de)\n",
    "    # reconvert text code to emojis\n",
    "    list_emoji = [emoji.emojize(x) for x in emojis_list_de]\n",
    "\n",
    "    if return_codes:\n",
    "        return emojis_list_de\n",
    "    else:\n",
    "        return list_emoji\n",
    "\n",
    "tweets_df['emoji'] = tweets_df['text'].apply(extract_emojis)\n",
    "tweets_df['emoji_text'] = tweets_df['text'].apply(extract_emojis, return_codes=True)\n",
    "\n",
    "tweets_df.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the final results from our extraction example and sort values according to mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emoji</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1258617080430997505</td>\n",
       "      <td>A Black New York State Senator (@zellnor4ny) a...</td>\n",
       "      <td>9151</td>\n",
       "      <td>[https://t.co/NoT8g4uAli]</td>\n",
       "      <td>[@zellnor4ny, @YourFavoriteASW]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1266956300908363776</td>\n",
       "      <td>NEW: A volunteer on Kushner's coronavirus resp...</td>\n",
       "      <td>9327</td>\n",
       "      <td>[https://t.co/jvs2h4IfNQ]</td>\n",
       "      <td>[@yabutaleb7]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[: A volunteer on Kushner's coronavirus respon...</td>\n",
       "      <td>[: A volunteer on Kushner's coronavirus respon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1260559563972960256</td>\n",
       "      <td>Wow! The Front Page @washingtonpost Headline r...</td>\n",
       "      <td>11591</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@washingtonpost]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1262940294305071104</td>\n",
       "      <td>it would appear that @vp was joking about carr...</td>\n",
       "      <td>11196</td>\n",
       "      <td>[https://t.co/hI9cO4lxcX]</td>\n",
       "      <td>[@vp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1261718681882693632</td>\n",
       "      <td>Very happy to present this unseen image of @ta...</td>\n",
       "      <td>10245</td>\n",
       "      <td>[https://t.co/3dzvynlUq3]</td>\n",
       "      <td>[@tarak9999, @DabbooRatnani]</td>\n",
       "      <td>[#HappyBirthdayNTR, #StayHomeStaySafe]</td>\n",
       "      <td>[üòé\\n\\nüì∏ By @DabbooRatnani \\n\\n#HappyBirthdayNT...</td>\n",
       "      <td>[:smiling_face_with_sunglasses:\\n\\n:camera_wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1256717572373913605</td>\n",
       "      <td>Update: Got her permission with a fuck yeah. T...</td>\n",
       "      <td>19289</td>\n",
       "      <td>[https://t.co/MqV0QJ0D8h]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1265624335898869760</td>\n",
       "      <td>Y'all, the mask goes OVER your nose.</td>\n",
       "      <td>19351</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1258599146522464256</td>\n",
       "      <td>Because if its Baghdad its okay for this to ha...</td>\n",
       "      <td>19457</td>\n",
       "      <td>[https://t.co/UdFy61zoT5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1266343312304324608</td>\n",
       "      <td>I gotta be honest the worst looting I've ever ...</td>\n",
       "      <td>19527</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1258018688231407618</td>\n",
       "      <td>‚ÄúOh shit my mask.‚Äù</td>\n",
       "      <td>15036</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "489  1258617080430997505  A Black New York State Senator (@zellnor4ny) a...   \n",
       "464  1266956300908363776  NEW: A volunteer on Kushner's coronavirus resp...   \n",
       "347  1260559563972960256  Wow! The Front Page @washingtonpost Headline r...   \n",
       "360  1262940294305071104  it would appear that @vp was joking about carr...   \n",
       "412  1261718681882693632  Very happy to present this unseen image of @ta...   \n",
       "..                   ...                                                ...   \n",
       "167  1256717572373913605  Update: Got her permission with a fuck yeah. T...   \n",
       "165  1265624335898869760               Y'all, the mask goes OVER your nose.   \n",
       "164  1258599146522464256  Because if its Baghdad its okay for this to ha...   \n",
       "163  1266343312304324608  I gotta be honest the worst looting I've ever ...   \n",
       "250  1258018688231407618                                 ‚ÄúOh shit my mask.‚Äù   \n",
       "\n",
       "     retweets                       urls                         mentions  \\\n",
       "489      9151  [https://t.co/NoT8g4uAli]  [@zellnor4ny, @YourFavoriteASW]   \n",
       "464      9327  [https://t.co/jvs2h4IfNQ]                    [@yabutaleb7]   \n",
       "347     11591                         []                [@washingtonpost]   \n",
       "360     11196  [https://t.co/hI9cO4lxcX]                            [@vp]   \n",
       "412     10245  [https://t.co/3dzvynlUq3]     [@tarak9999, @DabbooRatnani]   \n",
       "..        ...                        ...                              ...   \n",
       "167     19289  [https://t.co/MqV0QJ0D8h]                               []   \n",
       "165     19351                         []                               []   \n",
       "164     19457  [https://t.co/UdFy61zoT5]                               []   \n",
       "163     19527                         []                               []   \n",
       "250     15036                         []                               []   \n",
       "\n",
       "                                   hashtags  \\\n",
       "489                                      []   \n",
       "464                                      []   \n",
       "347                                      []   \n",
       "360                                      []   \n",
       "412  [#HappyBirthdayNTR, #StayHomeStaySafe]   \n",
       "..                                      ...   \n",
       "167                                      []   \n",
       "165                                      []   \n",
       "164                                      []   \n",
       "163                                      []   \n",
       "250                                      []   \n",
       "\n",
       "                                                 emoji  \\\n",
       "489                                                 []   \n",
       "464  [: A volunteer on Kushner's coronavirus respon...   \n",
       "347                                                 []   \n",
       "360                                                 []   \n",
       "412  [üòé\\n\\nüì∏ By @DabbooRatnani \\n\\n#HappyBirthdayNT...   \n",
       "..                                                 ...   \n",
       "167                                                 []   \n",
       "165                                                 []   \n",
       "164                                                 []   \n",
       "163                                                 []   \n",
       "250                                                 []   \n",
       "\n",
       "                                            emoji_text  \n",
       "489                                                 []  \n",
       "464  [: A volunteer on Kushner's coronavirus respon...  \n",
       "347                                                 []  \n",
       "360                                                 []  \n",
       "412  [:smiling_face_with_sunglasses:\\n\\n:camera_wit...  \n",
       "..                                                 ...  \n",
       "167                                                 []  \n",
       "165                                                 []  \n",
       "164                                                 []  \n",
       "163                                                 []  \n",
       "250                                                 []  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.sort_values(by='mentions', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise, let's clean text from urls, hashtags, mentions, and emojis for further text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    # find all URLs in text using regex\n",
    "    urls = re.findall(\"http[s]*\\S+\", text)\n",
    "    # iterate through the URLs and remove them\n",
    "    for url in urls:\n",
    "        text = text.replace(url, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    # find all hashtags in text using regex\n",
    "    hashtags = re.findall(\"@[a-zA-Z0-9_]{1,50}\", text)\n",
    "    # iterate through the hashtags and remove them\n",
    "    for hashtag in hashtags:\n",
    "        text = text.replace(hashtag, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_mentions(text):\n",
    "    # find all mentions in text using regex\n",
    "    mentions = re.findall(\"#[a-zA-Z0-9_]{1,50}\", text)\n",
    "    # iterate through the mentions and remove them\n",
    "    for mention in mentions:\n",
    "        text = text.replace(mention, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # find all emoji in text\n",
    "    emojis = extract_emojis(text, return_codes=False)\n",
    "    # iterate through the emojis and remove them\n",
    "    for emoji in emojis:\n",
    "        text = text.replace(emoji, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # create a cleaning pipeline \n",
    "    text = remove_urls(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_emojis(text)\n",
    "    return text\n",
    "\n",
    "tweets_df['cleaned_text'] = tweets_df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet: Very happy to present this unseen image of @tarak9999 .. I hope you all like it üòé\n",
      "\n",
      "üì∏ By @DabbooRatnani \n",
      "\n",
      "#HappyBirthdayNTR üéâ\n",
      "\n",
      "#StayHomeStaySafe üôèüèº https://t.co/3dzvynlUq3\n",
      "\n",
      "\n",
      "Cleaned Tweet: Very happy to present this unseen image of  .. I hope you all like it  \n"
     ]
    }
   ],
   "source": [
    "print('Original Tweet:', tweets_df.text.values[412])\n",
    "print('\\n\\nCleaned Tweet:', tweets_df.cleaned_text.values[412])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Extracting named entities\n",
    "\n",
    "A named entity is a real-life object which can be identified and denoted with a proper name. Named Entities can be a place, person, organization, time, object, or geographic entity. For example, named entities would be Joe Biden, New York city, and congress. Named entities are usually instances of entity instances. For example, Joe Biden is an instance of a politician/person, New York City is an instance of a place, and congress is as instance of an organization. \n",
    "\n",
    "**Named Entity Recognition** (NER) is the process of NLP for identifying and classifying named entities. The raw and structured text are used to find out named entities, which are classified into persons, organizations, places, money, time, etc. NER systems are developed with various linguistic approaches, as well as statistical and machine learning methods. \n",
    "\n",
    "NER model first identifies an entity and then categorizes the entity into the most suitable class. Some of the common types of Named Entities will be as follows and others can be found in the further example of a Wikipedia page text.\n",
    "\n",
    "1. Organisations : NASA, CERN\n",
    "\n",
    "2. Places: Istanbul, Germany\n",
    "\n",
    "3. Money: 1 Billion Dollars, 50 Euros\n",
    "\n",
    "4. Date: 24th January 2023, season 4\n",
    "\n",
    "5. Person: Richard Feynman, George Floyd\n",
    " \n",
    "<img src='../data/NER.png' style='height: 500px; float: left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "\n",
    "    \n",
    "For NLP tasks like NER, POS tagging, dependency parsing, word vectors and more, <a href=\"https://spacy.io/\">spaCy</a> has distinct features that provide clear advantage for processing text data and modeling. It is the most trending and advanced free open-source library for implementing NLP in Python nowadays. \n",
    "    \n",
    "An important thing about NER models is that their ability to understand Named Entities depending on the data they have been trained on. There are many applications of NER. NER can be used for content classification, the various Named Entities of a text can be collected, and based on that data, the content themes can be understood.\n",
    "    \n",
    "We can use spaCy very easily for NER tasks. However, we need to consider training our own data for research, commercial, and business specific needs, the spaCy model generally performs well for all types of text data. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's import necessary libraries and packages and start with a toy example from our tweets dataframe, which is the second line of the text column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Pence caught on hot mic delivering empty boxes of PPE for a PR stunt. \n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "# before loading it we need to install this module via: #!python -m spacy download en_core_web_sm\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the second tweet of our dataset\n",
    "raw_text = tweets_df.cleaned_text[1]\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we print the data on the Named Entities found in this raw text sample from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Pence PERSON\n",
      "PPE ORG\n"
     ]
    }
   ],
   "source": [
    "# extract the entities using the spacy objects previously defined in the\n",
    "NER_text = NER(raw_text)\n",
    "\n",
    "# show all the entities extracted from the text\n",
    "for word in NER_text.ents:\n",
    "    print(word.text, word.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-info'>\n",
    "<big><b>Insight</b></big>\n",
    "    \n",
    "Here, PPE is a context specific word to be labeled as organization. In the COVID-19 context like in our example, it stands for \"personal protective equipment\"; which is not an organization. On the other hand, as an abbreviation of the Philosophy, Politics, and Economics Society, PPE can be labeled as an organization.\n",
    "</div>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run NER on the full dataset and find out the output with Named Entities and who is the most cited Location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most cited location is: the Southern Border\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model with NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a dictionary to store the count of each location\n",
    "location_count = {}\n",
    "\n",
    "# Loop over each text and analyze it with spaCy's NER\n",
    "for text in tweets_df.cleaned_text:\n",
    "    doc = NER(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"LOC\":\n",
    "            # If the entity is a location, add it to the count dictionary\n",
    "            name = ent.text\n",
    "            if name in location_count:\n",
    "                location_count[name] += 1\n",
    "            else:\n",
    "                location_count[name] = 1\n",
    "\n",
    "# Find the person with the highest count\n",
    "most_cited_location = max(location_count, key=location_count.get)\n",
    "print(\"The most cited location is:\", most_cited_location)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the processed data for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(\"../data/top_500_retweeted_tweets_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0-GgEFeNJbn"
   ],
   "name": "Day 1: Introduction to Jupyter Notebook and Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
