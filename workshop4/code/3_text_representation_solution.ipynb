{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Computational Social Science methods with Python\n",
    "\n",
    "### Natural Language Processing - Text Representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "<b>In this Python notebook</b>, \n",
    "\n",
    "we will explore how to represent text data using bag of words and TF-IDF techniques. Text representation is a critical task in natural language processing (NLP), which involves converting raw text data into a numerical format that can be processed by machine learning algorithms. In this notebook, we will focus on two specific techniques for text representation: bag of words and TF-IDF.\n",
    "\n",
    "We will use a dataset of news articles obtained from a publicly available source, which consist of a collection of articles from different topics. We will preprocess the text data by cleaning the text, and then use the bag of words and TF-IDF techniques to represent the text data in a numerical format.\n",
    "\n",
    "By the end of this notebook, you will have a basic understanding of how to represent text data using bag of words and TF-IDF techniques. Let's get started!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Bag-of-words\n",
    "\n",
    "So far we have introduced what are tokenization, stemming, lemmatization, stop words, n-grams, and part of speech tagging. As we have seen, these are all preprocessing techniques that aims at cleaning, removing unnecessary information, and extracting structure from the text. As a last step of this section, we will introduce two techniques, the Bag-of-Words (BoW) approach and its natural extension, a technique called TF-IDF (Term Frequency-Inverse Document Frequency). These approaches combines the techniques described so far, and finally prepares the texts for the actual analysis, such as topic modeling, text classification, and sentiment analysis.\n",
    "\n",
    "\n",
    "In natural language processing (NLP), a **\"bag of words\"** is a representation of a text document that describes the occurrence of words in it. It is a simple and commonly used approach to convert text data into a numerical format that can be used for analysis and machine learning.\n",
    "The bag-of-words model ignores the order and structure of the text and only considers the frequency of occurrence of each word in the document. The resulting representation is a \"bag\" of words in which each word is represented as a separate feature, and the value of each feature is the count of the corresponding word in the document.\n",
    "\n",
    "The bag-of-words representation of a corpus is usually stored in a matrix called the **document-term matrix**, where each row represents a document and each column represents a term (i.e., a word). The value in each cell is the number of occurrences of the corresponding term in the corresponding document. The document-term matrix is generally a sparse matrix, meaning that it contains a large number of zero elements and few non-zero ones. Indeed, most documents only contain a small fraction of the possible words in a language, and most words occur in only a subset of the documents. This means that the vast majority of the entries in the document-term matrix are zero.\n",
    "\n",
    "We will now see how to derive the document-term matric using spaCy and Gensim. For this task, we will use another dataset that contains news articles. Let's import the data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/eleven-american...</td>\n",
       "      <td>11 American Troops Injured In Iran Attack On I...</td>\n",
       "      <td>WORLD NEWS</td>\n",
       "      <td>The United States military originally said no ...</td>\n",
       "      <td>Eric Beech, Reuters</td>\n",
       "      <td>2020-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/gus-kenwo...</td>\n",
       "      <td>Olympian Gus Kenworthy Burns Ivanka Trump: 'TF...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The first daughter led the U.S. delegation dur...</td>\n",
       "      <td>Alana Horowitz Satlin</td>\n",
       "      <td>2018-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/watch-ins...</td>\n",
       "      <td>WATCH: Inspiring Woman Living with Spinal Musc...</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>When Alyssa was just 5 months old, she was dia...</td>\n",
       "      <td>HooplaHa - Only Good News, Contributor\\nHoopla...</td>\n",
       "      <td>2013-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dad-deliv...</td>\n",
       "      <td>Brent Farrell, Dad, Knocked Down Locked Door T...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>A week before Henry's quick delivery, Katherin...</td>\n",
       "      <td>Jessica Samakow</td>\n",
       "      <td>2012-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/how-polit...</td>\n",
       "      <td>How Politically Correct Culture Influences My ...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>I may not abandon my child in the wilderness, ...</td>\n",
       "      <td>Toni Nagy, Contributor\\nwriter, podcaster, ton...</td>\n",
       "      <td>2014-01-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/eleven-american...   \n",
       "1  https://www.huffingtonpost.com/entry/gus-kenwo...   \n",
       "2  https://www.huffingtonpost.com/entry/watch-ins...   \n",
       "3  https://www.huffingtonpost.com/entry/dad-deliv...   \n",
       "4  https://www.huffingtonpost.com/entry/how-polit...   \n",
       "\n",
       "                                            headline    category  \\\n",
       "0  11 American Troops Injured In Iran Attack On I...  WORLD NEWS   \n",
       "1  Olympian Gus Kenworthy Burns Ivanka Trump: 'TF...      SPORTS   \n",
       "2  WATCH: Inspiring Woman Living with Spinal Musc...    WELLNESS   \n",
       "3  Brent Farrell, Dad, Knocked Down Locked Door T...   PARENTING   \n",
       "4  How Politically Correct Culture Influences My ...   PARENTING   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  The United States military originally said no ...   \n",
       "1  The first daughter led the U.S. delegation dur...   \n",
       "2  When Alyssa was just 5 months old, she was dia...   \n",
       "3  A week before Henry's quick delivery, Katherin...   \n",
       "4  I may not abandon my child in the wilderness, ...   \n",
       "\n",
       "                                             authors        date  \n",
       "0                                Eric Beech, Reuters  2020-01-17  \n",
       "1                              Alana Horowitz Satlin  2018-02-25  \n",
       "2  HooplaHa - Only Good News, Contributor\\nHoopla...  2013-05-20  \n",
       "3                                    Jessica Samakow  2012-04-09  \n",
       "4  Toni Nagy, Contributor\\nwriter, podcaster, ton...  2014-01-24  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# import the data\n",
    "news = pd.read_csv(\"../data/news_subset.csv\")\n",
    "news.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deriving the document-term matrix, we preprocess the articles using the technique described before: tokenization, lemmatization (or stemming), stop words and punctuation removal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove punctuation and special characters\n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "    text_clean = re.sub(pattern, \"\", text)\n",
    "\n",
    "    # remove numbers\n",
    "    pattern = r\"\\d+\"\n",
    "    text_clean = re.sub(pattern, \"\", text_clean)\n",
    "\n",
    "    # remove all non-ASCII characters\n",
    "    pattern = r\"[^\\x00-\\x7F]+\"\n",
    "    text_clean = re.sub(pattern, \"\", text_clean)\n",
    "\n",
    "    # remove new line characters\n",
    "    text_clean.replace(\"\\n\", \"\")\n",
    "\n",
    "    # remove empty spaces left by regex\n",
    "    text_clean = ' '.join(text_clean.split())\n",
    "    \n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenization(texts):\n",
    "    return [word_tokenize(text) for text in texts]\n",
    "\n",
    "\n",
    "def remove_stop_words(texts, stop_words=[]):\n",
    "    if stop_words == []:\n",
    "        stop_words = list(STOP_WORDS)\n",
    "    return [[word for word in doc if word.lower() not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def add_bigrams(texts):\n",
    "    bigrams = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "    return [bigrams[text] for text in texts]\n",
    "\n",
    "\n",
    "def stemming(texts):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [[stemmer.stem(word) for word in doc] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts):\n",
    "    texts_lemma = []\n",
    "    for text in texts:\n",
    "        doc = nlp(\" \".join(text)) \n",
    "        texts_lemma.append([token.lemma_ for token in doc])\n",
    "    return texts_lemma\n",
    "\n",
    "\n",
    "def pipeline(corpus):\n",
    "    print(\"Cleaning text...\")\n",
    "    corpus = [clean_text(text) for text in corpus]\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "    corpus = tokenization(corpus)\n",
    "\n",
    "    print(\"Lowercasing...\")\n",
    "    corpus = [[el.lower() for el in text] for text in corpus]\n",
    "\n",
    "    print(\"Stop Words removal...\")\n",
    "    corpus = remove_stop_words(corpus)\n",
    "\n",
    "    print(\"Extract bigrams...\")\n",
    "    corpus = add_bigrams(corpus)\n",
    "\n",
    "    print(\"Stemming...\")\n",
    "    corpus = stemming(corpus)\n",
    "\n",
    "    print(\"Stop Words removal after stemming...\")\n",
    "    corpus = remove_stop_words(corpus)\n",
    "\n",
    "    print(\"Removing tokens that are too short...\")\n",
    "    corpus = [[c for c in text if len(c) > 2] for text in corpus]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "Tokenization...\n",
      "Lowercasing...\n",
      "Stop Words removal...\n",
      "Extract bigrams...\n",
      "Stemming...\n",
      "Stop Words removal after stemming...\n",
      "Removing tokens that are too short...\n"
     ]
    }
   ],
   "source": [
    "# our corpus is title + description of the article\n",
    "corpus = []\n",
    "for index, row in news.iterrows():\n",
    "    corpus.append(row.headline + \". \" + row.short_description)\n",
    "corpus = np.array(corpus)\n",
    "\n",
    "# run the preprocessing pipeline\n",
    "corpus = pipeline(corpus)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to extract the document-term matrix from the preprocessed corpus. We will use and implementation of the BoW approach in Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# we create a dictionary\n",
    "dictionary = Dictionary(corpus)\n",
    "\n",
    "# we filter very common and very rare words\n",
    "#dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "\n",
    "# covert the corpus to bag of words format \n",
    "document_term_matrix = [dictionary.doc2bow(text) for text in corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell we have created a dictionary (i.e., a collection of all the words appearing in the corpus) and the document_term matrix. Let's see these outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the dictionary: 40358\n",
      "Dictionary first 5 elements (id, token): [(0, 'alasad'), (1, 'american'), (2, 'attack'), (3, 'base'), (4, 'erbil')]\n",
      "\n",
      "First document in bag-of-words format (raw): [(0, 1), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 3), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "First document in bag-of-words format (word, frequency): [['alasad', 1], ['american', 1], ['attack', 2], ['base', 2], ['erbil', 1], ['hurt', 1], ['injur', 1], ['iran', 2], ['iraq', 1], ['iraqi', 1], ['jan', 1], ['militari', 3], ['missil', 1], ['origin', 1], ['said', 1], ['service_memb', 1], ['troop', 1], ['united_st', 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in the dictionary: {0}\".format(len(dictionary)))\n",
    "print(\"Dictionary first 5 elements (id, token):\", list(dictionary.items())[:5])\n",
    "\n",
    "print(\"\\nFirst document in bag-of-words format (raw):\", document_term_matrix[0])\n",
    "print(\"First document in bag-of-words format (word, frequency):\", [[dictionary[id], freq] for id, freq in document_term_matrix[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also save the output for future analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"./output/dict_gensim.pkl\", \"wb\") as file:\n",
    "    pkl.dump(dictionary, file)\n",
    "\n",
    "with open(\"./output/corpus.pkl\", \"wb\") as file:\n",
    "    pkl.dump(corpus, file)\n",
    "\n",
    "with open(\"./output/document_term_matrix.pkl\", \"wb\") as file:\n",
    "    pkl.dump(document_term_matrix, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. TF-IDF\n",
    "\n",
    "Despite its simplicity, the BoW approach is a powerful technique to turn collections of text into a numerical format that can then be inputed to a variety of models for several applications, including topic modeling. Nonetheless, it has some limitations, such as:\n",
    "\n",
    "- Importance of rare words: The bag of words model assigns equal weight to all words in a document, regardless of their importance or rarity. \n",
    "- Discrimination of common words: The bag of words model assigns high weights to common words, which are not very informative and may not be discriminative for distinguishing between different documents. \n",
    "\n",
    "These limitations, can be corrected using the **Term Frequency-Inverse Document Frequency** (TF-IDF) matrix instead of the simple document-matrix. The idea behind TF-IDF is to assign a weight to each word in a document based on how frequently it occurs in the document and how important it is in the overall corpus. This weight is calculated by multiplying two factors:\n",
    "\n",
    "- **Term Frequency (TF)**: this is a measure of how often a word occurs in a document. It is calculated by dividing the number of occurrences of a word in a document by the total number of words in the document. The TF value for a word is high if it occurs very often in a document, and low if it occurs only a few times. In mathematical terms, the TF value for word $t$ in document $d$ is:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "tf(t, d) = \\frac{f_{t,d}}{\\sum_{t' \\in d}f_{t', d}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: This is a measure of how important a word is in a corpus. It is calculated by dividing the total number of documents in the corpus by the number of documents containing the word. The IDF value for a word is high if it occurs in a few documents and low if it occurs in many documents. In general, it is used the logarithm of the IDF factor. Indeed, if a word appears in only a very small number of documents, the resulting IDF value can be very large. This can lead to a situation where the TF-IDF weight of a word is dominated by its IDF value, even if its term frequency (TF) is relatively low. Taking the logarithmof the IDF value has the effect of compressing the range of possible IDF values and reducing the impact of very high IDF values. In mathematical terms, the TF value for word $t$ in a corpus $D$ of $N$ document is:\n",
    "$\n",
    "\\begin{align}\n",
    "idf(t, D) = log \\frac{N}{|\\{d \\in D : t \\in d\\}|}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "The TF-IDF weighting $w_{t, d, D}$ for a word $t$ is then calculated by multiplying the TF value and the IDF value for that word:\n",
    "$\n",
    "\\begin{align}\n",
    "w_{t, d, D} = tf(t, d) \\times idf(t, D)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "The higher the TF-IDF weighting, the more important the word is in the document or corpus. We can simply derive the TF-IDF matrix using Gensim starting from the bag-of-words representation of the preprocessed corpus previously obtained:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# fit TF-IDF model\n",
    "model = TfidfModel(document_term_matrix)\n",
    "tf_idf = model[document_term_matrix]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First document in TF-IDF format (raw): [(0, 0.3048256577720754), (1, 0.10267055678042152), (2, 0.249209864967932), (3, 0.29483215702461396), (4, 0.3048256577720754), (5, 0.15325778767051162), (6, 0.17868562942168534), (7, 0.33670071042346267), (8, 0.175367327477358), (9, 0.2023439280457968), (10, 0.18208408221404682), (11, 0.44882891302624855), (12, 0.20161212056132805), (13, 0.16219714724153844), (14, 0.0900998951919583), (15, 0.22500552666505164), (16, 0.17774119448204392), (17, 0.14904502951897228)]\n",
      "First document in TF-IDF format (word, frequency): [['alasad', 0.3048256577720754], ['american', 0.10267055678042152], ['attack', 0.249209864967932], ['base', 0.29483215702461396], ['erbil', 0.3048256577720754], ['hurt', 0.15325778767051162], ['injur', 0.17868562942168534], ['iran', 0.33670071042346267], ['iraq', 0.175367327477358], ['iraqi', 0.2023439280457968], ['jan', 0.18208408221404682], ['militari', 0.44882891302624855], ['missil', 0.20161212056132805], ['origin', 0.16219714724153844], ['said', 0.0900998951919583], ['service_memb', 0.22500552666505164], ['troop', 0.17774119448204392], ['united_st', 0.14904502951897228]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst document in TF-IDF format (raw):\", tf_idf[0])\n",
    "print(\"First document in TF-IDF format (word, frequency):\", [[dictionary[id], freq] for id, freq in tf_idf[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the TF-IDF object for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/tf_idf_gensim.pkl\", \"wb\") as file:\n",
    "    pkl.dump(tf_idf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0-GgEFeNJbn"
   ],
   "name": "Day 1: Introduction to Jupyter Notebook and Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
