{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmW7SSviHzBN"
   },
   "source": [
    "# Machine learning I\n",
    "## What is Machiene learning?\"\n",
    "### Some definitions\n",
    "* \"Field of study that gives computers the ability to learn without being explicitly programmed\" Arthur Samuel(1959)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/International_draughts.jpg\" alt=\"checkers\" width=\"200px\"/>\n",
    "\n",
    "* \"A computer program is said to learnfrom experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\" Tom Mitchell (1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8AowUiAHzBS"
   },
   "source": [
    "### Categories of Machine learning\n",
    "\n",
    "> Indented block\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![title](https://i.vas3k.ru/7w1.jpg)\n",
    "\n",
    "\n",
    "#### Supervised learning\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/gesiscss/WDCNLP/main/data/spam-filters.png)\n",
    "\n",
    "Supervised learning referes to modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data. Further subcategoris are classification tasks and regression tasks: in classification, the labels are discrete categories, while in regression, the labels are continuous quantities. \n",
    "\n",
    "#### Unsupervised learning\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gesiscss/WDCNLP/main/data/network.png\" alt=\"checkers\" width=\"200px\"/>\n",
    "\n",
    "Example: Networks clustering\n",
    "\n",
    "\n",
    "Unsupervised learning referes to modeling the features of a dataset without reference to any label. These models include tasks such as clustering or dimensionality reduction. Clustering algorithms identify distinct groups of data, while dimensionality reduction algorithms search for more succinct representations of the data.\n",
    "* Others are semi-supervised learning methods, reinforcement learning, recommender system, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IMdrpz9HzBT"
   },
   "source": [
    "## Supervised learning (Classification)\n",
    "We will first take a look at a simple classification task, in which you are given a set of labeled points and want to use these to classify some unlabeled points.\n",
    "\n",
    "$$\\mathbf{Data} = \\begin{bmatrix}\n",
    "    \\textbf{feature 1} & \\textbf{feature 2} & \\textbf{label}  \\\\\n",
    "    x_{1}^{(2)} & x_{2}^{(2)} & red \\\\\n",
    "    x_{1}^{(2)} & x_{2}^{(2)} & red \\\\\n",
    "        \\vdots & \\vdots & \\vdots \\\\\n",
    "    x_{1}^{(150)} & x_{2}^{(150)} & blue\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "![alt](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3qDaZokHzBT"
   },
   "source": [
    "This data set represents points in a 2-dimensional plane. Furthermore, each point is associated with one of two possible class labels (\"red\" or \"blue\").\n",
    "\n",
    "We will create a model that will let us decide whether to which of the two classes point belongs and assume that the two classes can be separated by drawing a straight line through between them.\n",
    "\n",
    "Here we have two-dimensional data: that is, we have two features for each point, represented by the (x,y) positions of the points on the plane. In addition, we have one of two class labels for each point, here represented by the colors of the points. From these features and labels, we would like to create a model that will let us decide whether a new point should be labeled \"blue\" or \"red.\"\n",
    "\n",
    "![alt](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-2.png)\n",
    "\n",
    "With this model we can now *predict* the classes of new unseen data.\n",
    "\n",
    "![alt](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-3.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3vS_hyCHzBU"
   },
   "source": [
    "## Introducing Scikit-Learn\n",
    "\n",
    "To make things less abstract let us begin with by using the Iris data set. This is a classic example dataset from statistics ([see](https://en.wikipedia.org/wiki/Iris_flower_data_set))\n",
    "\n",
    "Iris Setosa | Iris Versicolor  | Iris Virginica\n",
    "- | -  | - \n",
    "![alt](http://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg) | ![alt](http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/1920px-Iris_versicolor_3.jpg) | ![alt](http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/1920px-Iris_virginica.jpg)\n",
    "\n",
    "We will represent each induvidual flower sample as one row in our DataFrame, and the columns (features) represent the flower measurements in centimeters. We can represent the Iris dataset, consisting of 150 samples and 4 features, a 2-dimensional array or matrix $\\mathbb{R}^{150 \\times 4}$ in the following format:\n",
    "\n",
    "\n",
    "$$\\mathbf{Data} = \\begin{bmatrix}\n",
    "    \\textbf{feature 1} & \\textbf{feature 2} & \\textbf{feature 3} & \\dots  & \\textbf{label} \\\\\n",
    "    x_{1}^{(1)} & x_{2}^{(1)} & x_{3}^{(1)} & \\dots  & y^{(1)} \\\\\n",
    "    x_{1}^{(2)} & x_{2}^{(2)} & x_{3}^{(2)} & \\dots  & y^{(2)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{1}^{(150)} & x_{2}^{(150)} & x_{3}^{(150)} & \\dots  & y^{(150)}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "### Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TV1Qt1yyHzBV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kokm2EuhHzBX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/gesiscss/WDCNLP/main/data/iris.csv\", na_values=\"?\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0czfg0lHzBX"
   },
   "source": [
    "Let us do some basic inspection of our class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1iSeYVRHzBY"
   },
   "outputs": [],
   "source": [
    "df.species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKwX463wHzBY"
   },
   "source": [
    "inspect the dimensionality of the *DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njWqSg10HzBZ"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy1XoxQEHzBZ"
   },
   "source": [
    "and visualize the class with respect to two features *\"sepal_length\"* and *\"sepal_width\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fj4R8PlHzBa"
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2 [\"color\"] = df2.species.replace([\"versicolor\", \"virginica\", \"setosa\"], [\"red\", \"blue\", \"green\"])\n",
    "df2.plot.scatter(\"sepal_length\", \"sepal_width\", c=df2[\"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE9LwlmXHzBb"
   },
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "Scikit learn is not based on [Pandas DataFrame](https://www.geeksforgeeks.org/python-pandas-dataframe/), but on numpy arrays. NumPy is the package that underlies also Pandas, and we can get these arrays easily from a pandas frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5Hzj1d8HzBb"
   },
   "outputs": [],
   "source": [
    "val = df.values\n",
    "print (type(val))\n",
    "print (val[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBEHGxjZHzBb"
   },
   "source": [
    "To make a classification, we need separate arrays for the describing features, and the class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lMOIXvjHzBb"
   },
   "outputs": [],
   "source": [
    "labels = df['species'].values\n",
    "labels[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYvagEWRHzBc"
   },
   "source": [
    "Classifiers in this package expect the classes to be integers, so we need to transform the strings into integers. Fortunately, there are convenience functions available to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZJ19j0gHzBc"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "labels = preprocessing.LabelEncoder().fit_transform(labels)\n",
    "labels[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYfZL0IPHzBc"
   },
   "outputs": [],
   "source": [
    "features = df.drop(\"species\", axis=1).values\n",
    "features[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUkq54tnHzBd"
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPNfRKqUHzBd"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Gucjj91HzBd"
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IODOlSZwHzBd"
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(features)\n",
    "predicted[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft4T5-mtHzBd"
   },
   "source": [
    "### Evaluation\n",
    "For evaluating our model we test which instances have been predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnCwCXmnHzBe"
   },
   "outputs": [],
   "source": [
    "labels == predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydQtX8hMHzBe"
   },
   "source": [
    "The number of datapoints wich are correctly predicted is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUiN1mFSHzBe"
   },
   "outputs": [],
   "source": [
    "sum(labels==predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnHzph9RHzBe"
   },
   "outputs": [],
   "source": [
    "sum(labels==predicted) / len (labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieGNMBsHHzBe"
   },
   "source": [
    "### Splitting the Data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMUQIUokHzBf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy30M4WlHzBf"
   },
   "outputs": [],
   "source": [
    "print (len (features_train))\n",
    "print (len (features_test))\n",
    "print (len (labels_train))\n",
    "print (len (labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRviDvV6HzBf"
   },
   "source": [
    "We then fit the classifier on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-hrSr06HzBg"
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit (features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhgyUJ9jHzBg"
   },
   "source": [
    "and use it to predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yfs5A67_HzBg"
   },
   "outputs": [],
   "source": [
    "predicted_test = clf.predict (features_test)\n",
    "predicted_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F63lnaT7HzBg"
   },
   "source": [
    "The accuracy on the test set is then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtwCtWwmHzBg"
   },
   "outputs": [],
   "source": [
    "sum(labels_test==predicted_test) / len (labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5cGYBYUHzBh"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f__Ov_fLHzBh"
   },
   "source": [
    "\n",
    "sepal_length\tsepal_width\tpetal_length\tpetal_width\tspecies\n",
    "0\t5.1\t3.5\t1.4\t0.2\tsetosa\n",
    "1\t4.9\t3.0\t1.4\t0.2\tsetosa\n",
    "2\t4.7\t3.2\t1.3\t0.2\tsetosa\n",
    "3\t4.6\t3.1\t1.5\t0.2\tsetosa\n",
    "4\t5.0\t3.6\t1.4\t0.2\tsetosa\n",
    "We can extend this quivalently to other [evaluation measures](https://en.wikipedia.org/wiki/Precision_and_recall):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM3xjEa1HzBh"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(labels_test, predicted_test, labels=[1])\n",
    "print (\"Precision: \", precision) # if label 1 is predicted, how often is it really label 1\n",
    "print (\"Recall: \", recall) # How likely is the prediction of an instance with label 1 really label 1\n",
    "print (\"F_score: \", fscore) # harmonic mean of precision and recall\n",
    "print (\"support: \", support) # how often does this label occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsDZeZlYHzBh"
   },
   "source": [
    "### K-fold cross validation\n",
    "We will now randomly partition the data into k equal sized subsamples. We then retain a single subsample for testing our model and use the remaining k − 1 subsamples as training dat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uojd5eIHzBh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(features):\n",
    "    #print (test_index)\n",
    "    features_train = features[train_index]\n",
    "    labels_train = labels[train_index]\n",
    "    features_test = features [test_index]\n",
    "    labels_test = labels [test_index]\n",
    "    \n",
    "    clf.fit(features_train, labels_train)\n",
    "    predicted_test = clf.predict (features_test)\n",
    "    print(sum(labels_test==predicted_test) / len (labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAv09JyMHzBi"
   },
   "source": [
    "Sklearn provides the shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bty72TpuHzBi"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, features, labels, cv=20, scoring=\"accuracy\")\n",
    "print (scores)\n",
    "print (\"Accuracy according to cross-validation: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkUT0NKAHzBi"
   },
   "source": [
    "## Further learning resources\n",
    "* VanderPlas, Jake. *Python data science handbook: essential tools for working with data*. \" O'Reilly Media, Inc.\", 2016.\n",
    "* Gramfort, Alex and Mueller, Andreas *Scipy 2017 scikit-learn tutorial* \"SciPy\", 2017.\n",
    "* Guido, Sarah and Mueller, Andreas *Introduction to Machine Learning with Python: A Guide for Data Scientists* ([link](https://github.com/amueller/introduction_to_ml_with_python))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-qHc53iHzBi"
   },
   "source": [
    "#### Used resources\n",
    "* Wikipedia.org\n",
    "* Python Data Science Handbook\n",
    "* Kaggle\n",
    "* Google\n",
    "* A review on machine learning: trends and future prospects by Manish Kumar Aery and Chet Ram"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_lecture_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
