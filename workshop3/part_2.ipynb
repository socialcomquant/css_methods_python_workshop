{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huwt0uqOYugd"
   },
   "source": [
    "# Machine learning I\n",
    "## Unsupervised learning (Clustering)\n",
    "\n",
    "We will assume that our data has a form simmilar to the superviced setting. Our data consists of N samples with M features, a 2-dimensional array or matrix $\\mathbb{R}^{N \\times M}$ in the following format:\n",
    "\n",
    "$$\\mathbf{Data} = \\begin{bmatrix}\n",
    "    \\textbf{feature 1} & \\textbf{feature 2} & \\textbf{feature 3} & \\dots  & \\textbf{feature M} \\\\\n",
    "    x_{1}^{(1)} & x_{2}^{(1)} & x_{3}^{(1)} & \\dots  & x_{M}^{(1)} \\\\\n",
    "    x_{1}^{(2)} & x_{2}^{(2)} & x_{3}^{(2)} & \\dots  & x_{M}^{(2)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{1}^{(N)} & x_{2}^{(N)} & x_{3}^{(N)} & \\dots  & x_{M}^{(N)}\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2febF2rYugi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yha04I4-Yugk"
   },
   "source": [
    "We start by Creating some artificial dataset with M = 2 features and N = 1000 observations. We further assume that our data is drawn from 4 clusters (centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWQCxjlPYugl"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.make_blobs(n_samples=1000, n_features=2, centers=4, cluster_std=1.5, random_state=7)\n",
    "print (\"X:\\n\", X[:5])\n",
    "print (\"y:\\n\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZmQdHAPYugl"
   },
   "source": [
    "and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8XF89h7Yugm"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KztBuCVdYugm"
   },
   "source": [
    "Train a k-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvjz4YTlYugn"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clust = KMeans(n_clusters=4)\n",
    "y_cl = clust.fit_predict(X)\n",
    "y_cl[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tt8hJdzYugo"
   },
   "source": [
    "and visualize the predicted cluster memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImJi8JWmYugp"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztxVWPiuYugp"
   },
   "source": [
    "What happens if we have a 'wrong' number (e.g. 3 or 6) of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKtg1ym2Yugq"
   },
   "outputs": [],
   "source": [
    "# %load solutions/l2_num_clusters.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpeXGyoEYugq"
   },
   "source": [
    "## How can we find a good number of clusters k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHGcxTsiYugq"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsmI1mbOYugr"
   },
   "source": [
    "The [silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBNcMJbYYugr"
   },
   "outputs": [],
   "source": [
    "clust = KMeans(n_clusters=5)\n",
    "y_cl = clust.fit_predict(X)\n",
    "metrics.silhouette_score(X, y_cl, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJVtBDjoYugr"
   },
   "outputs": [],
   "source": [
    "ks = range(2,10)\n",
    "scores = []\n",
    "for k in ks:\n",
    "    clust = KMeans(n_clusters=k)\n",
    "    y_cl = clust.fit_predict(X)\n",
    "    scores.append(metrics.silhouette_score(X, y_cl, metric='euclidean'))\n",
    "plt.plot(ks,scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gbK9L4xYugs"
   },
   "source": [
    "Alternative scores are the [Calinski and Harabaz score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabaz_score.html) or \n",
    "[Rand index adjusted for chance](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html).\n",
    "\n",
    "## Other clustering methods:\n",
    "### 1) [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uexw1ccDYugs"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clust = DBSCAN(eps=1, min_samples=5)\n",
    "y_cl = clust.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42QPdRQaYugs"
   },
   "source": [
    "Note that there are some \"outliers\" that do not belong to any cluster!\n",
    "\n",
    "Which instances are the outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7PXZkDPYugt"
   },
   "outputs": [],
   "source": [
    "np.where(y_cl == -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcN99MK5Yugt"
   },
   "source": [
    "### 2) Use [Agglomerative Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) for documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBRO3DHwYugt"
   },
   "outputs": [],
   "source": [
    "# %load solutions/l2_AgglomerativeClustering.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T6AKOrjYugu"
   },
   "source": [
    "## Clustering the Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRdZ30gJYugu"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/gesiscss/WDCNLP/main/data/iris.csv\", na_values=\"?\")\n",
    "df[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9PUc54DYugu"
   },
   "outputs": [],
   "source": [
    "X =  df.drop(\"species\", axis=1).values\n",
    "X[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irSI4wX6Yugu"
   },
   "source": [
    "In a real example we should standardize the features. The [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) removes the mean and scales them to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX7KhlVnYugu"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_scaled[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EolFR0XYugv"
   },
   "outputs": [],
   "source": [
    "clust = KMeans(n_clusters=3)\n",
    "y_cl = clust.fit_predict(X_scaled)\n",
    "y_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ryg2JpNlYugv"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIZap23SYugv"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 2], X[:, 3], c=y_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ze54fB5Yugv"
   },
   "source": [
    "### Conducting a [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) on the Iris features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFWM5qlgYugv"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfrhROzSYugw"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:, 0], X[:, 1], c=y_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_lecture_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
